{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/gwtaylor/theano-rnn/blob/master/rnn.py\n",
    "# http://deeplearning.net/tutorial/mlp.html\n",
    "\n",
    "\"\"\" Vanilla RNN\"\"\"\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from sklearn.base import BaseEstimator\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import cPickle as pickle\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.ion()\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" DANIEL'S CODE\"\"\"\n",
    "\n",
    "allData = pd.read_csv('../household_power_consumption.txt',';',index_col=0,                      \n",
    "                      na_values='?',                      \n",
    "                      header=0,                      \n",
    "                      parse_dates=[[0, 1]],\n",
    "                      infer_datetime_format=True)\n",
    "groupedByH = allData.groupby(pd.TimeGrouper('H')).max()\n",
    "\n",
    "def splitDataset(dt):\n",
    "    y = dt.year\n",
    "    if(y>=2006 and y<=2008):\n",
    "        return 'training'\n",
    "    if(y==2009):\n",
    "        return 'validation'\n",
    "    if(y==2010):\n",
    "        return 'test'\n",
    "\n",
    "def removeNullRows(dataSet):\n",
    "    idxNAN = pd.isnull(dataSet).any(1).nonzero()[0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData = allData.dropna() # Delete the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splittedDataset = groupedByH.groupby(splitDataset)\n",
    "trainingSet = splittedDataset.get_group('training')\n",
    "validationSet = splittedDataset.get_group('validation')\n",
    "testSet = splittedDataset.get_group('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Sub_meter_1\"\"\"\n",
    "\n",
    "trainingSet = np.array(trainingSet.Sub_metering_1.values,dtype=theano.config.floatX)\n",
    "validationSet = np.array(validationSet.Sub_metering_1.values,dtype=theano.config.floatX)\n",
    "trainingSet = np.r_[trainingSet,validationSet]\n",
    "\n",
    "trainingSet = trainingSet[~np.isnan(trainingSet)]\n",
    "testSet = np.array(testSet.Sub_metering_1.values,dtype=theano.config.floatX)\n",
    "testSet = testSet[~np.isnan(testSet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taking only Global_active_power\n",
    "trainingSet = np.array(trainingSet.Global_active_power.values,dtype=theano.config.floatX)\n",
    "validationSet = np.array(validationSet.Global_active_power.values,dtype=theano.config.floatX)\n",
    "trainingSet = np.r_[trainingSet,validationSet]\n",
    "\n",
    "trainingSet = trainingSet[~np.isnan(trainingSet)]\n",
    "testSet = np.array(testSet.Global_active_power.values,dtype=theano.config.floatX)\n",
    "testSet = testSet[~np.isnan(testSet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = theano.Mode(linker='cvm')\n",
    "#mode = 'DEBUG_MODE'\n",
    "\n",
    "class RNN(object):\n",
    "    \"\"\"    Recurrent neural network class\n",
    "\n",
    "    Supported output types:\n",
    "    real : linear output units, use mean-squared error\n",
    "    binary : binary output units, use cross-entropy error\n",
    "    softmax : single softmax out, use cross-entropy error\n",
    "    \"\"\"\n",
    "    def __init__(self, input, n_in, n_hidden, n_out, activation=T.tanh,\n",
    "                 output_type='real', use_symbolic_softmax=False):\n",
    "\n",
    "        self.input = input\n",
    "        self.activation = activation\n",
    "        self.output_type = output_type\n",
    "\n",
    "        # when using HF, SoftmaxGrad.grad is not implemented\n",
    "        # use a symbolic softmax which is slightly slower than T.nnet.softmax\n",
    "        # See: http://groups.google.com/group/theano-dev/browse_thread/\n",
    "        # thread/3930bd5a6a67d27a\n",
    "        if use_symbolic_softmax:\n",
    "            def symbolic_softmax(x):\n",
    "                e = T.exp(x)\n",
    "                return e / T.sum(e, axis=1).dimshuffle(0, 'x')\n",
    "            self.softmax = symbolic_softmax\n",
    "        else:\n",
    "            self.softmax = T.nnet.softmax\n",
    "\n",
    "        # recurrent weights as a shared variable\n",
    "        W_init = np.asarray(np.random.uniform(size=(n_hidden, n_hidden),\n",
    "                                              low=-.01, high=.01),\n",
    "                                              dtype=theano.config.floatX)\n",
    "        self.W = theano.shared(value=W_init, name='W')\n",
    "        # input to hidden layer weights\n",
    "        W_in_init = np.asarray(np.random.uniform(size=(n_in, n_hidden),\n",
    "                                                 low=-.01, high=.01),\n",
    "                                                 dtype=theano.config.floatX)\n",
    "        self.W_in = theano.shared(value=W_in_init, name='W_in')\n",
    "\n",
    "        # hidden to output layer weights\n",
    "        W_out_init = np.asarray(np.random.uniform(size=(n_hidden, n_out),\n",
    "                                                  low=-.01, high=.01),\n",
    "                                                  dtype=theano.config.floatX)\n",
    "        self.W_out = theano.shared(value=W_out_init, name='W_out')\n",
    "\n",
    "        h0_init = np.zeros((n_hidden,), dtype=theano.config.floatX)\n",
    "        self.h0 = theano.shared(value=h0_init, name='h0')\n",
    "\n",
    "        bh_init = np.zeros((n_hidden,), dtype=theano.config.floatX)\n",
    "        self.bh = theano.shared(value=bh_init, name='bh')\n",
    "\n",
    "        by_init = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        self.by = theano.shared(value=by_init, name='by')\n",
    "\n",
    "        self.params = [self.W, self.W_in, self.W_out, self.h0,\n",
    "                       self.bh, self.by]\n",
    "\n",
    "        # for every parameter, we maintain it's last update\n",
    "        # the idea here is to use \"momentum\"\n",
    "        # keep moving mostly in the same direction\n",
    "        self.updates = {}\n",
    "        for param in self.params:\n",
    "            init = np.zeros(param.get_value(borrow=True).shape,\n",
    "                            dtype=theano.config.floatX)\n",
    "            self.updates[param] = theano.shared(init)\n",
    "\n",
    "        # recurrent function (using tanh activation function) and linear output\n",
    "        # activation function\n",
    "        def step(x_t, h_tm1):\n",
    "            h_t = self.activation(T.dot(x_t, self.W_in) + \\\n",
    "                                  T.dot(h_tm1, self.W) + self.bh)\n",
    "            y_t = T.dot(h_t, self.W_out) + self.by\n",
    "            return h_t, y_t\n",
    "\n",
    "        # the hidden state `h` for the entire sequence, and the output for the\n",
    "        # entire sequence `y` (first dimension is always time)\n",
    "        [self.h, self.y_pred], _ = theano.scan(step,\n",
    "                                               sequences=self.input,\n",
    "                                               outputs_info=[self.h0, None])\n",
    "\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "        self.L1 = 0\n",
    "        self.L1 += abs(self.W.sum())\n",
    "        self.L1 += abs(self.W_in.sum())\n",
    "        self.L1 += abs(self.W_out.sum())\n",
    "\n",
    "        # square of L2 norm ; one regularization option is to enforce\n",
    "        # square of L2 norm to be small\n",
    "        self.L2_sqr = 0\n",
    "        self.L2_sqr += (self.W ** 2).sum()\n",
    "        self.L2_sqr += (self.W_in ** 2).sum()\n",
    "        self.L2_sqr += (self.W_out ** 2).sum()\n",
    "\n",
    "        if self.output_type == 'real':\n",
    "            self.loss = lambda y: self.mse(y)\n",
    "        elif self.output_type == 'binary':\n",
    "            # push through sigmoid\n",
    "            self.p_y_given_x = T.nnet.sigmoid(self.y_pred)  # apply sigmoid\n",
    "            self.y_out = T.round(self.p_y_given_x)  # round to {0,1}\n",
    "            self.loss = lambda y: self.nll_binary(y)\n",
    "        elif self.output_type == 'softmax':\n",
    "            # push through softmax, computing vector of class-membership\n",
    "            # probabilities in symbolic form\n",
    "            self.p_y_given_x = self.softmax(self.y_pred)\n",
    "\n",
    "            # compute prediction as class whose probability is maximal\n",
    "            self.y_out = T.argmax(self.p_y_given_x, axis=-1)\n",
    "            self.loss = lambda y: self.nll_multiclass(y)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def mse(self, y):\n",
    "        # error between output and target\n",
    "        return T.mean((self.y_pred - y) ** 2)\n",
    "\n",
    "    def nll_binary(self, y):\n",
    "        # negative log likelihood based on binary cross entropy error\n",
    "        return T.mean(T.nnet.binary_crossentropy(self.p_y_given_x, y))\n",
    "\n",
    "    def nll_multiclass(self, y):\n",
    "        # negative log likelihood based on multiclass cross entropy error\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of time steps (call it T) in the sequence\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the sequence\n",
    "        over the total number of examples in the sequence ; zero one\n",
    "        loss over the size of the sequence\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_out.ndim:\n",
    "            raise TypeError('y should have the same shape as self.y_out',\n",
    "                ('y', y.type, 'y_out', self.y_out.type))\n",
    "\n",
    "        if self.output_type in ('binary', 'softmax'):\n",
    "            # check if y is of the correct datatype\n",
    "            if y.dtype.startswith('int'):\n",
    "                # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "                # represents a mistake in prediction\n",
    "                return T.mean(T.neq(self.y_out, y))\n",
    "            else:\n",
    "                raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetaRNN(BaseEstimator):\n",
    "    def __init__(self, n_in=5, n_hidden=50, n_out=5, learning_rate=0.01,\n",
    "                 n_epochs=100, L1_reg=0.00, L2_reg=0.00, learning_rate_decay=1,\n",
    "                 activation='tanh', output_type='real',\n",
    "                 final_momentum=0.9, initial_momentum=0.5,\n",
    "                 momentum_switchover=5,\n",
    "                 use_symbolic_softmax=False):\n",
    "        self.n_in = int(n_in)\n",
    "        self.n_hidden = int(n_hidden)\n",
    "        self.n_out = int(n_out)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.learning_rate_decay = float(learning_rate_decay)\n",
    "        self.n_epochs = int(n_epochs)\n",
    "        self.L1_reg = float(L1_reg)\n",
    "        self.L2_reg = float(L2_reg)\n",
    "        self.activation = activation\n",
    "        self.output_type = output_type\n",
    "        self.initial_momentum = float(initial_momentum)\n",
    "        self.final_momentum = float(final_momentum)\n",
    "        self.momentum_switchover = int(momentum_switchover)\n",
    "        self.use_symbolic_softmax = use_symbolic_softmax\n",
    "\n",
    "        self.ready()\n",
    "\n",
    "    def ready(self):\n",
    "        # input (where first dimension is time)\n",
    "        self.x = T.matrix()\n",
    "        # target (where first dimension is time)\n",
    "        if self.output_type == 'real':\n",
    "            self.y = T.matrix(name='y', dtype=theano.config.floatX)\n",
    "        elif self.output_type == 'binary':\n",
    "            self.y = T.matrix(name='y', dtype='int32')\n",
    "        elif self.output_type == 'softmax':  # only vector labels supported\n",
    "            self.y = T.vector(name='y', dtype='int32')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # initial hidden state of the RNN\n",
    "        self.h0 = T.vector()\n",
    "        # learning rate\n",
    "        self.lr = T.scalar()\n",
    "\n",
    "        if self.activation == 'tanh':\n",
    "            activation = T.tanh\n",
    "        elif self.activation == 'sigmoid':\n",
    "            activation = T.nnet.sigmoid\n",
    "        elif self.activation == 'relu':\n",
    "            activation = lambda x: x * (x > 0)\n",
    "        elif self.activation == 'cappedrelu':\n",
    "            activation = lambda x: T.minimum(x * (x > 0), 6)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.rnn = RNN(input=self.x, n_in=self.n_in,\n",
    "                       n_hidden=self.n_hidden, n_out=self.n_out,\n",
    "                       activation=activation, output_type=self.output_type,\n",
    "                       use_symbolic_softmax=self.use_symbolic_softmax)\n",
    "\n",
    "        if self.output_type == 'real':\n",
    "            self.predict = theano.function(inputs=[self.x, ],\n",
    "                                           outputs=self.rnn.y_pred,\n",
    "                                           mode=mode)\n",
    "        elif self.output_type == 'binary':\n",
    "            self.predict_proba = theano.function(inputs=[self.x, ],\n",
    "                                outputs=self.rnn.p_y_given_x, mode=mode)\n",
    "            self.predict = theano.function(inputs=[self.x, ],\n",
    "                                outputs=T.round(self.rnn.p_y_given_x),\n",
    "                                mode=mode)\n",
    "        elif self.output_type == 'softmax':\n",
    "            self.predict_proba = theano.function(inputs=[self.x, ],\n",
    "                        outputs=self.rnn.p_y_given_x, mode=mode)\n",
    "            self.predict = theano.function(inputs=[self.x, ],\n",
    "                                outputs=self.rnn.y_out, mode=mode)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def shared_dataset(self, data_xy):\n",
    "        \"\"\" Load the dataset into shared variables \"\"\"\n",
    "\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                            dtype=theano.config.floatX))\n",
    "\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                            dtype=theano.config.floatX))\n",
    "\n",
    "        if self.output_type in ('binary', 'softmax'):\n",
    "            return shared_x, T.cast(shared_y, 'int32')\n",
    "        else:\n",
    "            return shared_x, shared_y\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\" Return state sequence.\"\"\"\n",
    "        params = self._get_params()  # parameters set in constructor\n",
    "        weights = [p.get_value() for p in self.rnn.params]\n",
    "        state = (params, weights)\n",
    "        return state\n",
    "\n",
    "    def _set_weights(self, weights):\n",
    "        \"\"\" Set fittable parameters from weights sequence.\n",
    "\n",
    "        Parameters must be in the order defined by self.params:\n",
    "            W, W_in, W_out, h0, bh, by\n",
    "        \"\"\"\n",
    "        i = iter(weights)\n",
    "\n",
    "        for param in self.rnn.params:\n",
    "            param.set_value(i.next())\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\" Set parameters from state sequence.\n",
    "\n",
    "        Parameters must be in the order defined by self.params:\n",
    "            W, W_in, W_out, h0, bh, by\n",
    "        \"\"\"\n",
    "        params, weights = state\n",
    "        self.set_params(**params)\n",
    "        self.ready()\n",
    "        self._set_weights(weights)\n",
    "\n",
    "    def save(self, fpath='.', fname=None):\n",
    "        \"\"\" Save a pickled representation of Model state. \"\"\"\n",
    "        fpathstart, fpathext = os.path.splitext(fpath)\n",
    "        if fpathext == '.pkl':\n",
    "            # User supplied an absolute path to a pickle file\n",
    "            fpath, fname = os.path.split(fpath)\n",
    "\n",
    "        elif fname is None:\n",
    "            # Generate filename based on date\n",
    "            date_obj = datetime.datetime.now()\n",
    "            date_str = date_obj.strftime('%Y-%m-%d-%H:%M:%S')\n",
    "            class_name = self.__class__.__name__\n",
    "            fname = '%s.%s.pkl' % (class_name, date_str)\n",
    "\n",
    "        fabspath = os.path.join(fpath, fname)\n",
    "\n",
    "        logger.info(\"Saving to %s ...\" % fabspath)\n",
    "        file = open(fabspath, 'wb')\n",
    "        state = self.__getstate__()\n",
    "        pickle.dump(state, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        file.close()\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\" Load model parameters from path. \"\"\"\n",
    "        logger.info(\"Loading from %s ...\" % path)\n",
    "        file = open(path, 'rb')\n",
    "        state = pickle.load(file)\n",
    "        self.__setstate__(state)\n",
    "        file.close()\n",
    "\n",
    "    def fit(self, X_train, Y_train, X_test=None, Y_test=None,\n",
    "            validation_frequency=100):\n",
    "        \"\"\" Fit model\n",
    "\n",
    "        Pass in X_test, Y_test to compute test error and report during\n",
    "        training.\n",
    "\n",
    "        X_train : ndarray (n_seq x n_steps x n_in)\n",
    "        Y_train : ndarray (n_seq x n_steps x n_out)\n",
    "\n",
    "        validation_frequency : int\n",
    "            in terms of number of sequences (or number of weight updates)\n",
    "        \"\"\"\n",
    "        if X_test is not None:\n",
    "            assert(Y_test is not None)\n",
    "            self.interactive = True\n",
    "            test_set_x, test_set_y = self.shared_dataset((X_test, Y_test))\n",
    "        else:\n",
    "            self.interactive = False\n",
    "\n",
    "        train_set_x, train_set_y = self.shared_dataset((X_train, Y_train))\n",
    "\n",
    "        n_train = train_set_x.get_value(borrow=True).shape[0]\n",
    "        if self.interactive:\n",
    "            n_test = test_set_x.get_value(borrow=True).shape[0]\n",
    "\n",
    "        ######################\n",
    "        # BUILD ACTUAL MODEL #\n",
    "        ######################\n",
    "        logger.info('... building the model')\n",
    "\n",
    "        index = T.lscalar('index')    # index to a case\n",
    "        # learning rate (may change)\n",
    "        l_r = T.scalar('l_r', dtype=theano.config.floatX)\n",
    "        mom = T.scalar('mom', dtype=theano.config.floatX)  # momentum\n",
    "\n",
    "        cost = self.rnn.loss(self.y) \\\n",
    "            + self.L1_reg * self.rnn.L1 \\\n",
    "            + self.L2_reg * self.rnn.L2_sqr\n",
    "        \n",
    "        \"\"\"What is train_set_x? \"\"\"\n",
    "        compute_train_error = theano.function(inputs=[index, ],\n",
    "                                              outputs=self.rnn.loss(self.y),\n",
    "                                              givens={\n",
    "                                                  self.x: train_set_x[index],\n",
    "                                                  self.y: train_set_y[index]},\n",
    "                                                mode=mode )\n",
    "\n",
    "        if self.interactive:\n",
    "            compute_test_error = theano.function(inputs=[index, ],\n",
    "                        outputs=self.rnn.loss(self.y),\n",
    "                        givens={\n",
    "                            self.x: test_set_x[index],\n",
    "                            self.y: test_set_y[index]},\n",
    "                        mode=mode)\n",
    "\n",
    "        # compute the gradient of cost with respect to theta = (W, W_in, W_out)\n",
    "        # gradients on the weights using BPTT\n",
    "        gparams = []\n",
    "        for param in self.rnn.params:\n",
    "            gparam = T.grad(cost, param)\n",
    "            gparams.append(gparam)\n",
    "\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.rnn.params, gparams):\n",
    "            weight_update = self.rnn.updates[param]\n",
    "            upd = mom * weight_update - l_r * gparam\n",
    "            updates[weight_update] = upd\n",
    "            updates[param] = param + upd\n",
    "\n",
    "        # compiling a Theano function `train_model` that returns the\n",
    "        # cost, but in the same time updates the parameter of the\n",
    "        # model based on the rules defined in `updates`\n",
    "        train_model = theano.function(inputs=[index, l_r, mom],\n",
    "                                      outputs=cost,\n",
    "                                      updates=updates,\n",
    "                                      givens={\n",
    "                                          self.x: train_set_x[index],\n",
    "                                          self.y: train_set_y[index]},\n",
    "                                          mode=mode)\n",
    "\n",
    "        ###############\n",
    "        # TRAIN MODEL #\n",
    "        ###############\n",
    "        logger.info('... training')\n",
    "        epoch = 0\n",
    "\n",
    "        while (epoch < self.n_epochs):\n",
    "            epoch = epoch + 1\n",
    "            for idx in xrange(n_train):\n",
    "                effective_momentum = self.final_momentum \\\n",
    "                               if epoch > self.momentum_switchover \\\n",
    "                               else self.initial_momentum\n",
    "                example_cost = train_model(idx, self.learning_rate,\n",
    "                                           effective_momentum)\n",
    "\n",
    "                # iteration number (how many weight updates have we made?)\n",
    "                # epoch is 1-based, index is 0 based\n",
    "                iter = (epoch - 1) * n_train + idx + 1\n",
    "\n",
    "                if iter % validation_frequency == 0:\n",
    "                    # compute loss on training set\n",
    "                    train_losses = [compute_train_error(i)\n",
    "                                    for i in xrange(n_train)]\n",
    "                    this_train_loss = np.mean(train_losses)\n",
    "\n",
    "                    if self.interactive:\n",
    "                        test_losses = [compute_test_error(i)\n",
    "                                        for i in xrange(n_test)]\n",
    "                        this_test_loss = np.mean(test_losses)\n",
    "\n",
    "                        logger.info('epoch %i, seq %i/%i, tr loss %f '\n",
    "                                    'te loss %f lr: %f' % \\\n",
    "                        (epoch, idx + 1, n_train,\n",
    "                         this_train_loss, this_test_loss, self.learning_rate))\n",
    "                    else:\n",
    "                        logger.info('epoch %i, seq %i/%i, train loss %f '\n",
    "                                    'lr: %f' % \\\n",
    "                                    (epoch, idx + 1, n_train, this_train_loss,\n",
    "                                     self.learning_rate))\n",
    "\n",
    "            self.learning_rate *= self.learning_rate_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_real(trainingSet,testSet):\n",
    "    \"\"\" Test RNN with real-valued outputs. \"\"\"\n",
    "    n_hidden = 3\n",
    "    n_in = 3\n",
    "    n_out = 1 \n",
    "    n_steps = len(trainingSet)\n",
    "    n_seq = 5\n",
    "\n",
    "    np.random.seed(0)\n",
    "    # simple lag test\n",
    "    seq = np.random.randn(n_seq, n_steps, n_in)\n",
    "    seq = np.array(seq,dtype=theano.config.floatX)\n",
    "    \n",
    "    \n",
    "    \"\"\"ACTUAL DATA\"\"\"    \n",
    "    data_temp = np.array(trainingSet,dtype=theano.config.floatX)\n",
    "    seq = np.zeros((n_seq, n_steps, n_in))\n",
    "    seq = np.array(seq,dtype=theano.config.floatX)\n",
    "    \n",
    "    new_seq = np.zeros((n_seq, n_steps - (n_in) , n_in)) \n",
    "    for n in range(n_seq):\n",
    "        seq[n,:,0]=data_temp\n",
    "        seq[n,:,1]=np.r_[data_temp[1:] ,0] #np.r_[0,seq[n,:,0][1:] ] # data_temp #np.r_[data_temp[1:],0]\n",
    "        seq[n,:,2]=np.r_[data_temp[2:],0,0]  #np.r_[0,0,seq[n,:,0][2:]] # data_temp #np.r_[data_temp[2:],0,0]\n",
    "        new_seq[n,:,0] = seq[n,:,0][:-n_in]\n",
    "        new_seq[n,:,1] = seq[n,:,1][:-n_in]\n",
    "        new_seq[n,:,2] = seq[n,:,2][:-n_in]\n",
    "        \n",
    "    \n",
    "        #seq[1,:,0]=data_temp\n",
    "        #seq[1,:,1]=data_temp#np.r_[data_temp[1:],0]\n",
    "        #seq[1,:,2]=data_temp#np.r_[data_temp[2:],0,0]\n",
    "    \"\"\"end\"\"\" \n",
    "    \n",
    "    seq = np.array(new_seq,dtype=theano.config.floatX)\n",
    "    \n",
    "    \n",
    "    targets = np.zeros((n_seq, len(seq[0,:,0]), n_out))\n",
    "    targets = np.array(targets,dtype=theano.config.floatX)\n",
    "    \n",
    "    targets[:, :, 0] = data_temp[n_in:]  # delayed 1\n",
    "\n",
    "    model = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n",
    "                    learning_rate=0.001, learning_rate_decay=0.999,\n",
    "                    n_epochs=100, activation='tanh')\n",
    "\n",
    "    model.fit(seq, targets, validation_frequency=1000)\n",
    "    \n",
    "    #plt.plot(seq[0])\n",
    "    \n",
    "    guess = model.predict(seq[0])\n",
    "    plt.plot(targets[0])\n",
    "    plt.plot(guess, linestyle='--')    \n",
    "    plt.show()\n",
    "\n",
    "    print \"Training MSE\" #,guess.shape\n",
    "    target_temp = np.array(targets[0])\n",
    "    seq_temp = np.array(guess)\n",
    "    #print (target_temp - seq_temp)\n",
    "    print np.sum( (target_temp - seq_temp)**2) / (1.0*len( target_temp ))\n",
    "    \n",
    "    test_data = np.matrix(testSet,dtype=theano.config.floatX).T \n",
    "    oneline_test_data = test_data\n",
    "    for k in range(n_in-1):\n",
    "        test_data = np.c_[test_data,oneline_test_data]\n",
    "    test_data = np.array(test_data,dtype=theano.config.floatX)\n",
    "    #test_data = np.c_[test_data,test_data,test_data]\n",
    "    \n",
    "    guess = model.predict(test_data) #seq[0])\n",
    "    \n",
    "\n",
    "    print \"Test MSE\"\n",
    "    guess = np.array(guess)\n",
    "    oneline_test_data = np.array(oneline_test_data)\n",
    "    print np.sum( (guess - oneline_test_data)**2) / (1.0*len(guess))\n",
    "    \n",
    "    \n",
    "    print \"plotting \"\n",
    "    plt.plot(test_data) #targets[0])\n",
    "    plt.plot(guess, linestyle='--') \n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(seq[0])\n",
    "    ax1.set_title('input')\n",
    "    ax2 = plt.subplot(212)\n",
    "    true_targets = plt.plot(targets[0])\n",
    "\n",
    "    guess = model.predict(seq[0])\n",
    "    guessed_targets = plt.plot(guess, linestyle='--')\n",
    "    for i, x in enumerate(guessed_targets):\n",
    "        x.set_color(true_targets[i].get_color())\n",
    "    ax2.set_title('solid: true output, dashed: model output')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE\n",
      "153.806686658\n",
      "Test MSE\n",
      "48.2249082208\n",
      "plotting \n",
      "Elapsed time: 111.930000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyoheikoyama\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:227: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <type 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    t0 = time.time()\n",
    "    test_real(trainingSet[:300],testSet)\n",
    "    # problem takes more epochs to solve\n",
    "    #test_binary(multiple_out=True, n_epochs=2400)\n",
    "    #test_softmax(n_epochs=250)\n",
    "    print \"Elapsed time: %f\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2L, 4L, 3L)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.ones((2,4,3)) + 1\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10L,)\n"
     ]
    }
   ],
   "source": [
    "data_temp = np.arange(10)\n",
    "seq = np.zeros((2,len(data_temp),4))\n",
    "print seq[0,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  5.,  6.,  7.,  8.,  9.,  0.,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1\n",
    "#data_temp = data_temp[-2]\n",
    "seq[n,:,0]=data_temp\n",
    "seq[n,:,1]=np.r_[data_temp[1:] ,0] # data_temp #np.r_[data_temp[1:],0]\n",
    "seq[n,:,2]=np.r_[data_temp[2:],0,0]\n",
    "\n",
    "\n",
    "seq[n,:,0][:-2]\n",
    "seq[n,:,1][:-2]\n",
    "seq[n,:,2][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Null Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeNullRows(dataSet):\n",
    "    idxNAN = pd.isnull(dataSet).any(1).nonzero()[0]\n",
    "\n",
    "#since there are the Nan, we should remove it before the training\n",
    "#therefore, we split the traiing set as sequnces of series without Nan\n",
    "    start = 0\n",
    "    idxSequences = []\n",
    "    seqlen = []\n",
    "    for idx in idxNAN:\n",
    "        if(start < idx):\n",
    "            #print str(start) + '-' + str(idx-1)\n",
    "            idxSequences += range(start,idx)\n",
    "            seqlen += [idx-start]\n",
    "            start = idx+1\n",
    "        else:\n",
    "            start = start +1\n",
    "    #print str(start) + '-' + str(len(dataSet))\n",
    "    idxSequences += range(start,len(dataSet))\n",
    "    seqlen +=  [len(dataSet)-start]\n",
    "    #print idxSequences\n",
    "    return dataSet.iloc[idxSequences],seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
