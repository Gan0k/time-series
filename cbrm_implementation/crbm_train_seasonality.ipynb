{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import crbm as C\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the dataset in traininig,validation and test set\n",
    "def splitDataset(dt):\n",
    "    y = dt.year\n",
    "    if(y>=2006 and y<=2008):\n",
    "        return 'training'\n",
    "    if(y==2009):\n",
    "        return 'validation'\n",
    "    if(y==2010):\n",
    "        return 'test'\n",
    "\n",
    "#remove the null rows\n",
    "def removeNullRows(dataSet):\n",
    "    idxNAN = pd.isnull(dataSet).any(1).nonzero()[0]\n",
    "\n",
    "#since there are the Nan, we should remove it before the training\n",
    "#therefore, we split the traiing set as sequnces of series without Nan\n",
    "    start = 0\n",
    "    idxSequences = []\n",
    "    seqlen = []\n",
    "    for idx in idxNAN:\n",
    "        if(start < idx):\n",
    "            #print str(start) + '-' + str(idx-1)\n",
    "            idxSequences += range(start,idx)\n",
    "            seqlen += [idx-start]\n",
    "            start = idx+1\n",
    "        else:\n",
    "            start = start +1\n",
    "    #print str(start) + '-' + str(len(dataSet))\n",
    "    idxSequences += range(start,len(dataSet))\n",
    "    seqlen +=  [len(dataSet)-start]\n",
    "    #print idxSequences\n",
    "    return dataSet.iloc[idxSequences],seqlen\n",
    "\n",
    "#normalize the values\n",
    "def normalizeValues(dataSet):\n",
    "    return (dataSet - dataSet.mean())/ dataSet.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_history(data_idx_vec):\n",
    "    hist_idx = np.zeros((len(data_idx_vec),24),dtype = np.int32)\n",
    "    for k in range(len(data_idx_vec)):\n",
    "        count = 0\n",
    "        data_idx = data_idx_vec[k]\n",
    "        for i in range(7,0,-1):\n",
    "            for j in [-1,0,1]:\n",
    "                hist_idx[k,count] = data_idx - 24*i + j\n",
    "                count=count+1\n",
    "        for j in range(3,0,-1):\n",
    "            hist_idx[k,count] = data_idx-j\n",
    "            count = count +1\n",
    "    return hist_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_training(trainingSet_matrix,seqlenTR,n_hidden = 5,batch_size=24,training_epochs=200):\n",
    "\n",
    "    print '\\nN_HIDDEN='+str(n_hidden)\n",
    "    \n",
    "    #we fix the delay in 24 variable. They are:\n",
    "    # t-1 t t+1 in the 7 previous day\n",
    "    # t-3 t-2 t-1 in the actual day\n",
    "    delay = 21+3\n",
    "    \n",
    "    #learning rate\n",
    "    learning_rate = 0.001;\n",
    "    \n",
    "    # compute number of visible units\n",
    "    n_dim = trainingSet_matrix.shape[1]\n",
    "    \n",
    "    #create the shared variable for the training set\n",
    "    batchdata = theano.shared(np.asarray(trainingSet_matrix, dtype=theano.config.floatX))\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lvector()    # index to a [mini]batch\n",
    "    index_hist = T.lvector()  # index to history\n",
    "    x = T.matrix('x')  # the data\n",
    "    x_history = T.matrix('x_history')\n",
    "\n",
    "    #theano.config.compute_test_value='warn'\n",
    "    #x.tag.test_value = np.random.randn(batch_size, n_dim)\n",
    "    #x_history.tag.test_value = np.random.randn(batch_size, n_dim*delay)\n",
    "\n",
    "    # initialize storage for the persistent chain\n",
    "    # (state = hidden layer of chain)\n",
    "\n",
    "    # construct the CRBM class\n",
    "    crbm = C.CRBM(input=x, input_history=x_history, n_visible=n_dim, n_hidden=n_hidden, delay=delay)\n",
    "\n",
    "    # get the cost and the gradient corresponding to one step of CD-15\n",
    "    cost, updates = crbm.get_cost_updates(lr=learning_rate, k=1)\n",
    "\n",
    "    #we should skip the first week\n",
    "    h_in_week = 7*24+1\n",
    "    batchdataindex = []\n",
    "    last = 0\n",
    "    for s in seqlenTR:\n",
    "        batchdataindex += range(last + h_in_week, last + s)\n",
    "        last += s\n",
    "    permindex = np.array(batchdataindex)\n",
    "    n_train_batches = len(permindex)/ batch_size\n",
    "\n",
    "    train_crbm = theano.function([index, index_hist], cost,\n",
    "               updates=updates,\n",
    "               givens={\n",
    "                        x: batchdata[index],\n",
    "                        x_history: batchdata[index_hist].reshape((batch_size, delay * n_dim))\n",
    "                      },\n",
    "               name='train_crbm')\n",
    "\n",
    "    plotting_time = 0.\n",
    "    start_time = time.clock()\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            #print '\\n'\n",
    "            # indexing is slightly complicated\n",
    "            # build a linear index to the starting frames for this batch\n",
    "            # (i.e. time t) gives a batch_size length array for data\n",
    "            data_idx = permindex[batch_index * batch_size:(batch_index + 1)* batch_size]\n",
    "            #print batch_index\n",
    "            #print data_idx\n",
    "            # now build a linear index to the frames at each delay tap\n",
    "            \n",
    "            #hist_idx = np.array([data_idx - n for n in xrange(1, delay + 1)]).T\n",
    "            hist_idx = build_history(data_idx)\n",
    "            #print hist_idx\n",
    "            this_cost = train_crbm(data_idx, hist_idx.ravel())\n",
    "            #print batch_index, this_cost\n",
    "            mean_cost += [this_cost]\n",
    "\n",
    "        print '\\rTraining epoch %d, cost is ' % epoch, np.mean(mean_cost),\n",
    "\n",
    "    end_time = time.clock()\n",
    "\n",
    "    pretraining_time = (end_time - start_time)\n",
    "\n",
    "    print ('\\nTraining took %f minutes' % (pretraining_time / 60.))\n",
    "    \n",
    "    return crbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#validate on the whole validation set\n",
    "def my_evaluation(crbm,evaluationSet_matrix,seqlenEVAL,doPlot=False,start=0,end=100):\n",
    "    n_samples=1\n",
    "    delay = crbm.delay\n",
    "    \n",
    "    h_in_week = 7*24+1\n",
    "    data_idx = []\n",
    "    last = 0\n",
    "    for s in seqlenEVAL:\n",
    "        data_idx += range(last + h_in_week, last + s)\n",
    "        last += s\n",
    "        \n",
    "    data_idx = np.asarray(data_idx)\n",
    "    orig_data = np.asarray(evaluationSet_matrix[data_idx],dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "    hist_idx = build_history(data_idx)\n",
    "    \n",
    "    hist_idx = hist_idx.ravel()\n",
    "    \n",
    "\n",
    "    orig_history = np.asarray(evaluationSet_matrix[hist_idx].reshape((len(data_idx), crbm.delay * crbm.n_visible)),dtype=theano.config.floatX)\n",
    "\n",
    "    generated_series = crbm.generate(orig_data, orig_history, n_samples=n_samples,n_gibbs=30)\n",
    "\n",
    "    MSE=[None]*crbm.n_visible\n",
    "    SMAPE=[None]*crbm.n_visible\n",
    "    for i in range(crbm.n_visible):\n",
    "        plotGEN = generated_series[:,n_samples-1,i]\n",
    "        if(doPlot):\n",
    "            plt.subplot(crbm.n_visible, 1, i+1)\n",
    "            plt.plot(plotGEN[start:end])\n",
    "            plt.plot(evaluationSet_matrix[start:end,i])  \n",
    "\n",
    "        MSE[i] = np.sum(np.power(plotGEN - orig_data[:,i],2))/(len(orig_data))\n",
    "        SMAPE[i] = np.sum(np.abs(plotGEN - orig_data[:,i]) / (np.abs(plotGEN) + np.abs(orig_data[:,i]))) / len(orig_data) *100                                                                                               \n",
    "    plt.show()\n",
    "    return MSE,SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addSeasonality(dataSet):\n",
    "    #day and month of the batchdata\n",
    "    dataSet_matrix = dataSet.values\n",
    "    \n",
    "    idx_train = dataSet.index\n",
    "    dow = idx_train.dayofweek\n",
    "    m = idx_train.month\n",
    "    h = idx_train.hour\n",
    "    d = idx_train.dayofyear\n",
    "\n",
    "    season_year = np.cos(((24 * (d-1) + h)*2*np.pi/(365*24-1))+3*np.pi/2)\n",
    "    season_week = np.cos((dow-1)*2*np.pi/6)\n",
    "    season_day = np.cos(h*2*np.pi/23)\n",
    "\n",
    "    #now create a matrix s.t. the column are seasonYear | seasonWeek | seasonDay | allOtherData\n",
    "    dataSet_matrix = np.column_stack((season_day,dataSet_matrix))\n",
    "    dataSet_matrix = np.column_stack((season_week,dataSet_matrix))\n",
    "    dataSet_matrix = np.column_stack((season_year,dataSet_matrix))\n",
    "    return dataSet_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Global_active_power      float64\n",
       "Global_reactive_power    float64\n",
       "Voltage                  float64\n",
       "Global_intensity         float64\n",
       "Sub_metering_1           float64\n",
       "Sub_metering_2           float64\n",
       "Sub_metering_3           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data from file\n",
    "allData = pd.read_csv('../household_power_consumption.txt',';',index_col=0,na_values='?',header=0,parse_dates=[[0, 1]],infer_datetime_format=True)\n",
    "\n",
    "allData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reduce the number of data coputing the max of each hour\n",
    "groupedByH = allData.groupby(pd.TimeGrouper('H')).max()\n",
    "#groupedByH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splittedDataset = groupedByH.groupby(splitDataset)\n",
    "\n",
    "#split dataset\n",
    "trainingSet = splittedDataset.get_group('training')\n",
    "validationSet = splittedDataset.get_group('validation')\n",
    "testSet = splittedDataset.get_group('test')\n",
    "\n",
    "#remove null values\n",
    "trainingSet,seqlenTR = removeNullRows(trainingSet)\n",
    "validationSet,seqlenVAL = removeNullRows(validationSet)\n",
    "testSet,seqlenTE = removeNullRows(testSet)\n",
    "\n",
    "#normaliza all values with 0 mean and 1 std. dev.\n",
    "trainingSet = normalizeValues(trainingSet)\n",
    "validationSet = normalizeValues(validationSet)\n",
    "testSet = normalizeValues(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data for the training\n",
    "trainingSet_matrix = trainingSet.values\n",
    "\n",
    "# compute number of visible units\n",
    "n_dim = trainingSet_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build validation set\n",
    "\n",
    "validationSet_matrix = validationSet.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N_HIDDEN=3\n",
      "Training epoch 999, cost is  2.10875647406 \n",
      "Training took 24.017207 minutes\n",
      "Generating frame 0\n",
      "\n",
      "N_HIDDEN=5\n",
      "Training epoch 999, cost is  1.80263074746 \n",
      "Training took 24.224040 minutes\n",
      "Generating frame 0\n",
      "\n",
      "N_HIDDEN=7\n",
      "Training epoch 999, cost is  1.96598852487 \n",
      "Training took 20.118550 minutes\n",
      "Generating frame 0\n",
      "\n",
      "N_HIDDEN=10\n",
      "Training epoch 999, cost is  2.03142605122 \n",
      "Training took 24.629604 minutes\n",
      "Generating frame 0\n",
      "\n",
      "N_HIDDEN=20\n",
      "Training epoch 999, cost is  2.13788273875 \n",
      "Training took 25.844320 minutes\n",
      "Generating frame 0\n",
      "\n",
      "N_HIDDEN=50\n",
      "Training epoch 999, cost is  2.22804198592 \n",
      "Training took 32.793645 minutes\n",
      "Generating frame 0\n",
      "\n",
      "N_HIDDEN=100\n",
      "Training epoch 999, cost is  2.30530199352 \n",
      "Training took 37.830083 minutes\n",
      "Generating frame 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "crbm.py:316: UserWarning: Updating an `OrderedUpdates` with a non-ordered dictionary with 2+ elements could make your code non-deterministic\n",
      "  ), axis=1)})\n"
     ]
    }
   ],
   "source": [
    "#choose the best setting\n",
    "n_hidden_values = [3,5,7,10,20,50,100]\n",
    "\n",
    "store_MSE = np.zeros((len(n_hidden_values),n_dim))\n",
    "store_SMAPE = np.zeros_like(store_MSE)\n",
    "store_crbm_Model = [None for i in range(len(n_hidden_values))]\n",
    "for idx_nh in range(len(n_hidden_values)):\n",
    "    nh = n_hidden_values[idx_nh]\n",
    "    crbmMdl = my_training(trainingSet_matrix,seqlenTR,n_hidden=nh,batch_size=24,training_epochs=1000)\n",
    "    MSE,SMAPE = my_evaluation(crbmMdl,validationSet_matrix,seqlenVAL)\n",
    "\n",
    "    store_crbm_Model[idx_nh] = crbmMdl\n",
    "    store_MSE[idx_nh,:] = MSE\n",
    "    store_SMAPE[idx_nh,:] = SMAPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST N HIDDEN = 3\n",
      "BEST MSE = 0.75025792574\n"
     ]
    }
   ],
   "source": [
    "best_idx_nh = store_MSE.mean(axis=1).argmin()\n",
    "best_nh_MSE = n_hidden_values[best_idx_nh]\n",
    "print 'BEST N HIDDEN = '+str(best_nh_MSE)\n",
    "print 'BEST MSE = '+str(store_MSE[best_idx_nh,:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST N HIDDEN = 5\n",
      "BEST SMAPE = 42.0938964663\n"
     ]
    }
   ],
   "source": [
    "best_idx_nh = store_SMAPE.mean(axis=1).argmin()\n",
    "best_nh_SMAPE = n_hidden_values[best_idx_nh]\n",
    "print 'BEST N HIDDEN = '+str(best_nh_SMAPE)\n",
    "print 'BEST SMAPE = '+str(store_SMAPE[best_idx_nh,:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N_HIDDEN=3\n",
      "Training epoch 999, cost is  2.11124506385 \n",
      "Training took 316.102039 minutes\n"
     ]
    }
   ],
   "source": [
    "#concatanate training and validation to obtain a bigger valid\n",
    "trainingSet_BIG_matrix = np.concatenate((trainingSet_matrix,validationSet_matrix))\n",
    "seqlenTR_BIG = seqlenTR + seqlenVAL\n",
    "\n",
    "best_crbm = my_training(trainingSet_BIG_matrix,seqlenTR_BIG,n_hidden=best_nh_MSE,batch_size=2,training_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating frame 0\n"
     ]
    }
   ],
   "source": [
    "#build test set\n",
    "testSet_matrix = testSet.values\n",
    "\n",
    "best_MSE,best_SMAPE = my_evaluation(best_crbm,testSet_matrix,seqlenTE,doPlot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.54236776282169941,\n",
       " 0.91522123667161859,\n",
       " 0.2936655395071277,\n",
       " 0.54412603005524407,\n",
       " 0.59715155318611968,\n",
       " 0.36573225249480418,\n",
       " 0.97884942771533767]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47.023793136672268,\n",
       " 66.098769009894738,\n",
       " 43.446789822545782,\n",
       " 47.019433981945085,\n",
       " 10.211582603925882,\n",
       " 21.444027053454139,\n",
       " 53.500950504805743]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
